# Tianchi_O2O_freshman_competition
天池O2O新手赛    AUC=0.7961 排名前段时间有30+，现在估计40+，随着后面越来越多人提交且结果会越来越好，如果不持续做的话排名会掉的.目前第一名的AUC有0.811+

     Data_preprocess.py 数据预处理部分，包括数据集打标和数据集划分;
     feat_section.py    特征区间的特征工程代码
     label_section.py   预测区间的特征工程代码
     time_gap_count.py  预测区间的某些时间差特征和leak特征
     model.py           xgb,lgb模型训练和使用RFE做特征选择

## 一.数据集划分
   使用时间窗口划分法划分数据
   
   | |特征区间|预测区间|
   |:---|:---|:---|
   |测试集|20160501~20160630|20160701~20160731|
   |训练集|20160401~20160531|20160601~20160630|
   |验证集|20160301~20160430|20160501~20160530|
   
   其中训练集与线下验证集可以做交叉验证。最后对测试集做预测时，将训练集与验证集合并一起预测
   
 ## 二.特征工程
   特征工程的重点是构建用户，商家，优惠券三大特征群，以及 用户-商家，用户-优惠券，商家-优惠券 三个交叉特征群。
   每个特征群主要特征有：
   
   统计特征（最大/最小/平均值/比率 等）  
   排序特征（各个实体对时间，距离，折扣率等的排序）  
   时间特征（日期，时间差等）  
   
   预测区间的特征特征区间的特征的相关性较强，因为对于此类时间序列相关的问题，越靠近预测时间，特征的相关性越强，并且，预测区间还有leakage特征可以用来上分，即使用未来的数据。当然，仅限于比赛使用，实际业务中是无法使用的。
   
## 三.模型训练
   模型主要是使用lgb模型，较快，大概5分钟。xgboost的精度较高，但是训练时间太长，大概30+分钟.
   
   P.S 一些个人见解：从用户画像的角度来看，统计特征和组合特征，主要分别刻画了用户，商家，优惠券的行为，比如，用户领券次数，商家的热度，优惠券的流行度等等。但是，排序特征，更多地从时间角度，和用户心理角度去考虑。比如说，距离领券时间越近，消费的欲望越强，因为如果领取了优惠券而迟迟没有消费，可能用户本身也忘记了这张优惠券的存在。同时，还有对距离的排序，线下商家与用户的距离越近，肯定要比远的商家消费的概率要大的。不可否认，距离本身也是一种排序。但为什么另外写了一个基于距离的排序特征，排序特征的重要性要比距离的重要性要强呢？我觉得应该和算法本身有关的。xgboost里的树模型在做分裂的时候，选取特征是有一个打分函数决定的，分值越高，信息增益越大。特征分裂时，并不是简单地按照样本个数选取分位点，而是，将特征转换为二阶导数值，再去选取分位点。连续均匀值比连续不均匀值信息增益要好。我觉得是因为，泰勒一阶与二阶展开式带有非线性项（二次项），数值的选取对其信息增益的计算会造成剧烈的扰动。而且，特征转换的本身，也是将目标函数引导到另一个特征取值更加平滑的解空间，更有利于求得最优解。
